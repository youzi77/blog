## 分布式、微服务理解

### 	主要解决问题

​	分布式和微服务主要解决问题是传统项目（单体应用架构）无法满足当前复杂环境要求，从而转向分布式和微服务，分布式和微服务理解如下。而设计过程当中分布式和微服务主要的设计架构有两个理论**CAP定律**和**BASE理论**,而且必须要遵守这两种理论，下面会详细说明两种理论。

	1. 分布式

    解决不停机快速迭代，发布、平滑发布、不受时间内影响

    - 分散的是压力

    - 分布式可以说是微服务，反之不一定成立

    - 分布式服务架构。基于去中心化的分布式服务框架与技术，考虑系统架构和服务治理；

	2. 微服务

    解决客户业务系统扩展，模块拆分，分别部署不同服务器（也可以部署在同一台），业务功能不产生相互影响，一个模块宕机不影响其他模块

    - 分散的是能力
    - 微服务架构通常是分布式服务架构，反之不一定成立
    - 微服务架构。微服务架构可以看作是面向服务架构和分布式服务架构的拓展，使用更细粒度的服务和一组设计准则来考虑大规模的复杂系统架构设计。

	3. 设计原则

    - CAP理论

      **CAP理论是分布式架构中重要理论，分布式系统的架构设计需遵循这一原则。**

      - **一致性(Consistency)**：即所有节点在同一时间具有相同的数据。

        在设计或者部署一个分布式系统时，假设存在两个Redis库，分别是Redis主和Redis从库，用户在往数据库插入一笔记录 ，那么会在保存到数据库的同时也会实时的插入到Redis缓存中，并且主从库保持同步，如果这时候向数据库查询，在网络延迟的情况下，在同一时刻，Redis主和Redis从库查询到的数据应该是一致性的，每个节点的数据是相同的，不能允许有脏读。

      - **可用性(Availability)**：保证每个请求不管成功或者失败都有响应。、

        假设在分布式部署中，有以下集群部署节点，如果`Tomcat1`出现故障，`Nginx`会自动做故障转移，自动将请求转移到`Tomcat2`和`Tomcat3`，`Nginx`故障转移可帮助我们实现集群环境的可用性

      - **分隔容忍(Partition tolerance) ：** 系统中任意信息的丢失或失败不会影响系统的继续运作

        分区容错性是指系统能够容忍节点之间的网络通信的故障，假设我们有一台Tomcat服务器，部署在全国各地，像浙江节点、上海节点、武汉节点，但是可能出现其中某个节点由于网络故障不能使用，无法进行通讯；一般来说，现实中分区容错是无法避免的，因此可以认为 CAP的 P总是成立。CAP定理告诉我们，剩下的 C 和 A 无法同时做到。

      **常见的CAP定律组合**

      ​	在分布式、微服务系统中，`CAP`定律只能在`A`和`C`之间做取舍，三者无法同时兼顾，其中可以容忍网络之间出现的通讯故障，只能`CP`和`AP`二选一。

      	- **CP：**当网络出现故障之后，只能保证数据一致性，但是不能保证可用性(`Zookeeper`集群过半数节点宕机时保证了数据一致性，为CP实现者），意味着服务不能用；
      	- **AP：** 当网络出现故障之后，不能保证数据一致性， 但是能保证可用性(比如`Eureka`的采用去中心化思想和自我保护机制，实现了`AP`)，意味着可允许短暂的数据不一致性问题，但最终需达到一致，其他节点可正常提供服务，如此保证了可用性但牺牲了一致性；

    - BASE理论

      BASE是Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是： 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

      - 基本可用

        分布式系统出现不可预知故障是，允许损失一部分可用行，但是不代表系统不可用。主要是有两种表现一是响应时间是损失，查询结果的时间增加；二是系统功能损失保证系统稳定的前提下，进行消费降级

      - 软状态

        系统中的数据存在中间状态，并且该状态不会影响系统整体可用性，表现为不同节点数据副本之间数据同步存在延迟

      - 最终一致性

        各个数据副本之间，经过同步保证达到同一个状态，并不是说一定是实时，存在短暂的延迟

	4. Eureka、Zookeeper、Consul、Nacos的相同点和区别

    - Eureka作为注册中心原理：

      **保证AP可用**，`Eureka`采用去中心化思想，如果使用`Eureka`实现集群，它的每个节点都是相等的；没有主从概念 ，采用相互注册原理 ，如果客户端访问的`eureka`宕机之后会自动切换其他`eureka`节点；只要能够有一台`eureka`服务正常使用，整个服务注册中心就使用；

    - Zookeeper作为注册中心原理：

      **保证CP（一致性）**，Zookeeper 在设计时就紧遵CP原则，即任何时候对 Zookeeper 的访问请求能得到一致的数据结果。当我们在集群集群环境中，如果某个节点发生了宕机的情况，需要重新实现选举策略，选举的过程中需要一些时间。问题在于，选举leader的时间太长，30~120s，而且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪，Zookeeper 底层采用zab协议保持集群之后的每个节点数据一致性，这就导致Zookeeper不能正常使用。如果Zookeeper集群中半数以上服务器节点不可用，那么将无法处理该请求。因此，Zookeeper 不能保证服务可用性，但保证了数据的一致性。

    -  Consul作为注册中心原理：

      Consul主要是强一致性；

      服务注册相比Eureka会稍慢一些。因为Consul的raft协议要求必须过半数的节点都写入成功才认为注册成功

      Leader挂掉时，重新选举期间整个consul不可用。保证了强一致性但牺牲了可用性。

    - Nacos作为注册中心原理：

      nacos底层默认采用ap模式，但从1.0版本后采用的是ap+cp混合模式的注册中心，两种都支持，可进行切换，cp模式时底层采用Raft算法进行选举，采用cp模式时，需要进行选举Leader角色。

    - 主流的注册中心

    |                 | Nacos                      | Eureka      | Consul            | CoreDNS    | Zookeeper  |
    | --------------- | -------------------------- | ----------- | ----------------- | ---------- | ---------- |
    | 一致性协议      | CP+AP                      | AP          | CP                | ---        | CP         |
    | 健康检查        | TCP/HTTP/MYSQL/Client Beat | Client Beat | TCP/HTTP/gRPC/Cmd | ---        | Keep Alive |
    | 负载均衡策略    | 权重/metadata/Selector     | Ribbon      | Fabio             | RoundRobin | ---        |
    | 雪崩保护        | 有                         | 有          | 无                | 无         | 无         |
    | 自动注销实例    | 支持                       | 支持        | 不支持            | 不支持     | 支持       |
    | 访问协议        | HTTP/DNS                   | HTTP        | HTTP/DNS          | DNS        | TCP        |
    | 监听支持        | 支持                       | 支持        | 支持              | 不支持     | 支持       |
    | 多数据中心      | 支持                       | 支持        | 支持              | 不支持     | 不支持     |
    | 跨注册中心同步  | 支持                       | 不支持      | 支持              | 不支持     | 不支持     |
    | springcloud集成 | 支持                       | 支持        | 支持              | 不支持     | 支持       |
    | Dubbo集成       | 支持                       | 不支持      | 不支持            | 不支持     | 支持       |
    | K8S集成         | 支持                       | 不支持      | 支持              | 支持       | 不支持     |

	5. springboot和dubbo

    - 定位区别： Dubbo它的关注点主要在于服务的调用，流量分发、流量监控和熔断。

      ​					Spring Cloud 诞生于微服务架构时代，考虑的是微服务治理的方方面面。

    - 模块区别: Dubbo主要分为服务注册中心，服务提供者，服务消费者，还有管控中心；

       				SpringCloud则是一个完整的分布式一站式框架，它有着一样的服务注册中心，服务提供者，服务消费者，管控台，断路器，分布式配置服务，消息总线，以及服务追踪等；

    | 功能         | Dubbo         | springcloud                 |
    | ------------ | ------------- | --------------------------- |
    | 服务注册中心 | Zookeeper     | SpringCloud Netflix Eureka  |
    | 服务调用方式 | RPC           | REST Api                    |
    | 服务监控     | Dubbo-monitor | Spring Boot Admin           |
    | 断路器       | 不完善        | SpringCloud Netflix Hystrix |
    | 服务网关     | 无            | SpringCloud Netflix Zuul    |
    | 分布式配置   | 无            | SpringCloud Config          |
    | 服务跟踪     | 无            | SpringCloud Sleuth          |
    | 消息总线     | 无            | SpringCloud Bus             |
    | 数据流       | 无            | SpringCloud Stream          |
    | 批量任务     | 无            | SpringCloud task            |

    